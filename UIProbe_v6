import time
import csv
import math
import statistics
import os
from datetime import datetime
from collections import defaultdict
from playwright.sync_api import sync_playwright, TimeoutError as PlaywrightTimeoutError
import pandas as pd

# ================= CONFIG =================
URL_FILE = "urls.txt"

TEST_DURATION_MINUTES = 30
DELAY_BETWEEN_URLS_SEC = 5
BUCKET_SIZE_MINUTES = 5

RAW_RESULTS_FILE = "results.csv"
ERROR_LOG_FILE = "errors.csv"
SUMMARY_REPORT_FILE = "summary_report.csv"
MERGED_BUCKET_REPORT_FILE = "bucketed_performance_report.csv"

SCREENSHOT_DIR = "error_screenshots"
# ==========================================


# ---------- Utility Functions ----------

def load_urls(file_path):
    if file_path.endswith(".csv"):
        return pd.read_csv(file_path)["url"].dropna().tolist()
    else:
        with open(file_path, "r") as f:
            return [line.strip() for line in f if line.strip()]


def percentile(data, pct):
    if not data:
        return -1
    data = sorted(data)
    k = (len(data) - 1) * (pct / 100)
    f = math.floor(k)
    c = math.ceil(k)
    if f == c:
        return data[int(k)]
    return data[f] + (data[c] - data[f]) * (k - f)


def get_time_bucket(ts):
    return ts.replace(
        minute=(ts.minute // BUCKET_SIZE_MINUTES) * BUCKET_SIZE_MINUTES,
        second=0,
        microsecond=0
    )


def safe_filename(name):
    return name.replace("https://", "").replace("http://", "").replace("/", "_")


# ---------- Scenario Observer ----------

class ScenarioObserver:
    def __init__(self):
        self.start_time = None
        self.scenario_name = None

    def start_scenario(self, name: str):
        self.scenario_name = name
        self.start_time = time.time()

    def end_scenario(self, page):
        if not self.start_time:
            return None

        duration_ms = int((time.time() - self.start_time) * 1000)

        fcp = page.evaluate("""
        () => {
          const e = performance.getEntriesByName('first-contentful-paint')[0];
          return e ? e.startTime : -1;
        }
        """)

        lcp = page.evaluate("""
        () => new Promise(resolve => {
          new PerformanceObserver((list) => {
            const entries = list.getEntries();
            resolve(entries[entries.length - 1].startTime);
          }).observe({ type: 'largest-contentful-paint', buffered: true });
        })
        """)

        cls = page.evaluate("""
        () => {
          let value = 0;
          new PerformanceObserver((list) => {
            for (const e of list.getEntries()) {
              if (!e.hadRecentInput) value += e.value;
            }
          }).observe({ type: 'layout-shift', buffered: true });
          return value;
        }
        """)

        self.start_time = None
        return duration_ms, fcp, lcp, cls


# ---------- Main Logic ----------

def main():
    os.makedirs(SCREENSHOT_DIR, exist_ok=True)

    urls = load_urls(URL_FILE)
    end_time = time.time() + (TEST_DURATION_MINUTES * 60)

    timings = defaultdict(list)
    bucketed_timings = defaultdict(lambda: defaultdict(list))
    bucketed_lcp_timings = defaultdict(lambda: defaultdict(list))

    scenario_observer = ScenarioObserver()

    with open(RAW_RESULTS_FILE, "w", newline="") as raw_file, \
         open(ERROR_LOG_FILE, "w", newline="") as error_file:

        raw_writer = csv.writer(raw_file)
        error_writer = csv.writer(error_file)

        raw_writer.writerow([
            "timestamp_utc",
            "entity",
            "entity_type",
            "status",
            "duration_ms",
            "fcp_ms",
            "lcp_ms",
            "cls",
            "error_type",
            "error_message",
            "screenshot_path"
        ])

        error_writer.writerow([
            "timestamp_utc",
            "entity",
            "entity_type",
            "error_type",
            "error_message",
            "screenshot_path"
        ])

        with sync_playwright() as p:
            browser = p.chromium.launch(headless=True)
            context = browser.new_context()

            while time.time() < end_time:
                for url in urls:
                    if time.time() >= end_time:
                        break

                    page = context.new_page()
                    now = datetime.utcnow()

                    status = "SUCCESS"
                    error_type = ""
                    error_message = ""
                    screenshot_path = ""

                    duration_ms = -1
                    fcp = -1
                    lcp = -1
                    cls = 0.0

                    try:
                        scenario_observer.start_scenario(url)
                        page.goto(url, wait_until="load", timeout=60000)
                        result = scenario_observer.end_scenario(page)

                        if result:
                            duration_ms, fcp, lcp, cls = result

                    except PlaywrightTimeoutError as e:
                        status = "FAILURE"
                        error_type = "TIMEOUT"
                        error_message = str(e)

                    except Exception as e:
                        status = "FAILURE"
                        error_type = "NAVIGATION_ERROR"
                        error_message = str(e)

                    if status != "SUCCESS":
                        filename = f"{now.strftime('%Y%m%dT%H%M%S')}_{safe_filename(url)}_{error_type}.png"
                        screenshot_path = os.path.join(SCREENSHOT_DIR, filename)
                        try:
                            page.screenshot(path=screenshot_path, full_page=True)
                        except Exception:
                            screenshot_path = ""

                        error_writer.writerow([
                            now.isoformat(),
                            url,
                            "URL",
                            error_type,
                            error_message,
                            screenshot_path
                        ])
                        error_file.flush()

                    raw_writer.writerow([
                        now.isoformat(),
                        url,
                        "URL",
                        status,
                        duration_ms,
                        int(fcp),
                        int(lcp),
                        round(cls, 3),
                        error_type,
                        error_message,
                        screenshot_path
                    ])
                    raw_file.flush()

                    if status == "SUCCESS" and duration_ms > 0:
                        timings[url].append(duration_ms)
                        bucket = get_time_bucket(now)
                        bucketed_timings[url][bucket].append(duration_ms)
                        if lcp > 0:
                            bucketed_lcp_timings[url][bucket].append(lcp)

                    page.close()
                    time.sleep(DELAY_BETWEEN_URLS_SEC)

            browser.close()

    # ---------- Summary Report ----------
    with open(SUMMARY_REPORT_FILE, "w", newline="") as summary_file:
        writer = csv.writer(summary_file)
        writer.writerow([
            "entity",
            "avg_duration_ms",
            "p90_duration_ms",
            "max_duration_ms",
            "min_duration_ms",
            "sample_count"
        ])

        for entity, values in timings.items():
            writer.writerow([
                entity,
                int(statistics.mean(values)),
                int(percentile(values, 90)),
                max(values),
                min(values),
                len(values)
            ])

    # ---------- Bucketed Report ----------
    with open(MERGED_BUCKET_REPORT_FILE, "w", newline="") as bucket_file:
        writer = csv.writer(bucket_file)
        writer.writerow([
            "bucket_start_time_utc",
            "entity",
            "p90_duration_ms",
            "avg_duration_ms",
            "p90_lcp_ms",
            "avg_lcp_ms",
            "sample_count"
        ])

        for entity in bucketed_timings:
            for bucket in sorted(bucketed_timings[entity]):
                lt = bucketed_timings[entity][bucket]
                lcp_t = bucketed_lcp_timings[entity].get(bucket, [])

                writer.writerow([
                    bucket.isoformat(),
                    entity,
                    int(percentile(lt, 90)),
                    int(statistics.mean(lt)),
                    int(percentile(lcp_t, 90)) if lcp_t else -1,
                    int(statistics.mean(lcp_t)) if lcp_t else -1,
                    len(lt)
                ])


if __name__ == "__main__":
    main()
